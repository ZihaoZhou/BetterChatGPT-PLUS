{
  "data": [
    {
      "id": "Claude-3.7-Sonnet",
      "provider": "Anthropic",
      "context_length": 200000,
      "pricing": { "prompt": 0.003, "completion": 0.015, "image": 0.003, "request": 0 },
      "is_stream_supported": true,
      "description": "Anthropic's Claude 3.7 Sonnet, a hybrid-reasoning model with 200k context; offers top-tier accuracy and supports vision input and file attachments.",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Claude-3.7-Sonnet-Reasoning",
      "provider": "Anthropic",
      "context_length": 200000,
      "pricing": { "prompt": 0.003, "completion": 0.015, "image": 0.003, "request": 0 },
      "is_stream_supported": true,
      "description": "Anthropic's Claude 3.7 Sonnet (Reasoning mode) with an extended 200k context; maximizes step-by-step reasoning and accepts images and file inputs.",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Claude-3.5-Haiku",
      "provider": "Anthropic",
      "context_length": 200000,
      "pricing": { "prompt": 0.0008, "completion": 0.004, "image": 0.0008, "request": 0 },
      "is_stream_supported": true,
      "description": "Anthropic's Claude 3.5 Haiku model, a fast 200k-context assistant; supports vision and file inputs while delivering quick, reliable responses.",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Claude-3.5-Sonnet",
      "provider": "Anthropic",
      "context_length": 200000,
      "pricing": { "prompt": 0.003, "completion": 0.015, "image": 0.003, "request": 0 },
      "is_stream_supported": true,
      "description": "Anthropic's Claude 3.5 Sonnet with a 200k-token window, optimized for complex tasks; includes support for image understanding and file uploads.",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Claude-3-Sonnet",
      "provider": "Anthropic",
      "context_length": 200000,
      "pricing": { "prompt": 0.003, "completion": 0.015, "image": 0.003, "request": 0 },
      "is_stream_supported": true,
      "description": "Anthropic's Claude 3 Sonnet with a 200k-token context window; supports image inputs and file attachments for multimodal queries.",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "DeepSeek-R1",
      "provider": "DeepSeek",
      "context_length": 128000,
      "pricing": { "prompt": 0.00055, "completion": 0.00219, "image": 0.00055, "request": 0 },
      "is_stream_supported": true,
      "description": "DeepSeek R1 reasoning model (open-source, ~70B params) with 128k context; excels at step-by-step logic and math. Text-only (no image or file input).",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Command-R-Plus",
      "provider": "Cohere",
      "context_length": 131072,
      "pricing": { "prompt": 0.003, "completion": 0.015, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Cohere's Command R+ model (128k context) for robust instruction-following and retrieval-augmented tasks; text-only (no vision or file support).",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "DeepSeek-R1-Distill",
      "provider": "DeepSeek",
      "context_length": 128000,
      "pricing": { "prompt": 0.00020, "completion": 0.00080, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "DeepSeek R1 distilled variant (smaller dense model) offering efficient reasoning at lower cost. Text-only (no vision or file capabilities).",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "DeepSeek-R1-FW",
      "provider": "DeepSeek",
      "context_length": 128000,
      "pricing": { "prompt": 0.00055, "completion": 0.00219, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "DeepSeek R1 served via FireWorks (128k context) – an open reasoning model comparable to top-tier closed models. No support for image or file input.",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "DeepSeek-V3-FW",
      "provider": "DeepSeek",
      "context_length": 128000,
      "pricing": { "prompt": 0.00027, "completion": 0.00110, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "DeepSeek V3 (FireWorks hosted) – a 671B-parameter MoE chat model with 128k context for general-purpose Q&A. Text-only (no vision or file support).",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Deepseek-v3-T",
      "provider": "DeepSeek",
      "context_length": 64000,
      "pricing": { "prompt": 0.00040, "completion": 0.00130, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "DeepSeek V3 Turbo – a speed-optimized 64k-context version of DeepSeek V3 for faster responses. Text-only model without vision or file input support.",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "GPT-4o",
      "provider": "OpenAI",
      "context_length": 128000,
      "pricing": { "prompt": 0.00015, "completion": 0.00030, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "OpenAI's GPT-4o (omni) model with 128k context, multilingual and multimodal (text, image, audio) capabilities. Vision input is disabled here.",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "GPT-4o-Mini",
      "provider": "OpenAI",
      "context_length": 128000,
      "pricing": { "prompt": 0.00015, "completion": 0.00030, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "GPT-4o Mini – a smaller, cost-efficient variant of GPT-4o (128k context). Offers faster responses at lower cost. Text-only (no support for image/file input).",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Gemini-1.5-Flash-Search",
      "provider": "Google",
      "context_length": 1048576,
      "pricing": { "prompt": 0.00027, "completion": 0.00110, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Gemini 1.5 Flash + Search – an augmented version integrating retrieval capabilities. Provides quick answers using 1M-token context and search knowledge. (Text-only model.)",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Gemini-1.5-Flash",
      "provider": "Google",
      "context_length": 1048576,
      "pricing": { "prompt": 0.00027, "completion": 0.00110, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Gemini 1.5 Flash",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Gemini-1.5-Pro",
      "provider": "Google",
      "context_length": 2097152,
      "pricing": { "prompt": 0.00050, "completion": 0.00150, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Google's Gemini 1.5 Pro – a larger multimodal model with 2M-token input capacity (8k output). Excels at complex reasoning and broad knowledge. (Vision features not available here.)",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Gemini-1.5-Pro-Search",
      "provider": "Google",
      "context_length": 2097152,
      "pricing": { "prompt": 0.00050, "completion": 0.00150, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Gemini 1.5 Pro + Search – combines the power of Gemini Pro with retrieval augmentation. Handles 2M-token context for knowledge-intensive queries. No built-in vision or file support.",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Gemini-2.0-Flash",
      "provider": "Google",
      "context_length": 1048576,
      "pricing": { "prompt": 0.00040, "completion": 0.00130, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Gemini 2.0 Flash – Google's latest fast, multimodal model (1M-token input, 8k output). Offers improved reasoning and native tool use. (Text output only; no direct image/file input here.)",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Gemini-2.0-Flash-Lite",
      "provider": "Google",
      "context_length": 1048576,
      "pricing": { "prompt": 0.00020, "completion": 0.00065, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Gemini 2.0 Flash-Lite – a lighter, cost-efficient version of Gemini 2.0 Flash with the same 1M context window. Suited for lower-latency tasks. Text-only (no vision or file handling).",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Llama-3-70b-Groq",
      "provider": "Meta",
      "context_length": 4096,
      "pricing": { "prompt": 0.00018, "completion": 0.00018, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Meta's Llama 3 (70B) optimized on Groq hardware for speed. Provides strong chat and reasoning capabilities on a standard 4k context. Text-only model (no image or file support).",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Llama-3.1-405B-FW-128k",
      "provider": "Meta",
      "context_length": 128000,
      "pricing": { "prompt": 0.00300, "completion": 0.00900, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Llama 3.1 405B Instruct (FireWorks) – an extremely large 405B-parameter Meta model with 128k context. Excels in complex understanding. Text-only (no vision/file inputs).",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Llama-3.2-90B-FW-131k",
      "provider": "Meta",
      "context_length": 131000,
      "pricing": { "prompt": 0.00204, "completion": 0.00204, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Llama 3.2 90B Vision (FireWorks) – Meta's 90B multimodal model with ~131k context support. Strong in both language and visual reasoning (vision disabled in this environment).",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Llama-3.3-70B-FW",
      "provider": "Meta",
      "context_length": 4096,
      "pricing": { "prompt": 0.00039, "completion": 0.00039, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Llama 3.3 70B Instruct (FireWorks) – an improved 70B Meta model for text generation and reasoning, with enhanced performance over Llama 3.1. Supports text only (no image or file input).",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Llama-4-Maverick-17B-128E",
      "provider": "Meta",
      "context_length": 1000000,
      "pricing": { "prompt": 0.00050, "completion": 0.00077, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Meta's Llama 4 Maverick – a 400B total parameter model (17B active) with 128 experts. Features native multimodality with 1M token context window. Excels at reasoning, coding, and complex tasks.",
      "top_provider": {
        "context_length": 1000000,
        "is_moderated": false,
        "max_completion_tokens": 1000000
      }
    },
    {
      "id": "Llama-4-Scout-17B-16E",
      "provider": "Meta",
      "context_length": 10000000,
      "pricing": { "prompt": 0.00011, "completion": 0.00034, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Meta's Llama 4 Scout – a 109B total parameter model (17B active) with 16 experts. Supports an industry-leading 10M token context window with native multimodality. Can run on a single H100 GPU with quantization.",
      "top_provider": {
        "context_length": 10000000,
        "is_moderated": false,
        "max_completion_tokens": 10000000
      }
    },
    {
      "id": "Mistral-Large-2",
      "provider": "Mistral",
      "context_length": 4096,
      "pricing": { "prompt": 0.00017, "completion": 0.00017, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Upstage Mistral Large 2 (22B+ params) – a high-performance single-GPU model for enterprise tasks. Default 4k context. Text generation only (no vision or file handling).",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Mixtral-8x7b-Groq",
      "provider": "Mistral",
      "context_length": 4096,
      "pricing": { "prompt": 0.00018, "completion": 0.00018, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Mixtral 8×7B ensemble on Groq – a mixture of eight 7B Mistral models for enhanced output quality. 4k context per query. Text-only output (no support for vision or files).",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "QwQ-32B-Preview-T",
      "provider": "Alibaba",
      "context_length": 32768,
      "pricing": { "prompt": 0.00018, "completion": 0.00020, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Alibaba Qwen QwQ 32B (preview, turbo) – a 32k-context instruct model specialized for chat. Delivers strong multilingual answers. Text-only (no image or file inputs).",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Qwen2.5-72B-Instruct",
      "provider": "Alibaba",
      "context_length": 128000,
      "pricing": { "prompt": 0.00013, "completion": 0.00040, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Alibaba Qwen2.5 72B Instruct – a 128k-context instruction-tuned model with state-of-the-art knowledge and coding ability. Text generation only (no vision or file input here).",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "Qwen2.5-Coder-32B",
      "provider": "Alibaba",
      "context_length": 33000,
      "pricing": { "prompt": 0.00007, "completion": 0.00016, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "Alibaba Qwen2.5 Coder 32B – a code-specialized 32B model (33k context) excelled in programming tasks. Cost-effective for coding assistance. No vision or file input support.",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "o3-mini",
      "provider": "OpenAI",
      "context_length": 200000,
      "pricing": { "prompt": 0.0011, "completion": 0.0044, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "OpenAI o3-mini – a cost-efficient reasoning model (released 2025) with 200k context. Optimized for STEM reasoning and coding. No built-in vision or file input support.",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    },
    {
      "id": "o3-mini-high",
      "provider": "OpenAI",
      "context_length": 200000,
      "pricing": { "prompt": 0.0011, "completion": 0.0044, "image": 0, "request": 0 },
      "is_stream_supported": true,
      "description": "OpenAI o3-mini (High effort) – the o3-mini model running with extra thorough reasoning, using up to 200k context. Excels in complex problem solving. Text-only model (no vision/file).",
      "top_provider": {
        "context_length": 200000,
        "is_moderated": false,
        "max_completion_tokens": 200000
      }
    }
  ] 
}